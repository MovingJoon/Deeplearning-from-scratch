{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MovingJoon/Deeplearning-from-scratch/blob/master/Chapter_4%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDa7taj5FUXc"
      },
      "source": [
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "from functions import softmax, cross_entropy_error, sigmoid\n",
        "from gradient import numerical_gradient\n",
        "#손실함수 (오차제곱합, 교차엔트로피)\n",
        "def sum_squres_error(y,t):          #오차제곱합\n",
        "    return 0.5* np.sum((y-t)**2)\n",
        "\n",
        "def cross_entropy_error(y,t):       #교차 엔트로피\n",
        "    if y.ndim == 1:                 #데이터가 1개인 행렬이라면\n",
        "        t=t.reshape(1,t.size)       #1차원 배열로 전환\n",
        "        y=y.reshape(1,y.size)       #1차원 배열로 전환\n",
        "    batch_size= y.shape[0]          #데이터 갯수 60,000\n",
        "    return -np.sum(t*np.log(y+1e-7))/batch_size\n",
        "\n",
        "# 미분\n",
        "def function(x):\n",
        "    return x[0]**2+x[1]**2\n",
        "\n",
        "# 기울기 구하기(편미분)\n",
        "def numerical_diff(f,x):\n",
        "    h=1e-4 #0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        temp=x[idx]     #x[0,1,2,3...]\n",
        "        x[idx]=temp+h   #x+h\n",
        "        result1=f(x)    #f(x+h)\n",
        "\n",
        "        x[idx]=temp-h   #x-h\n",
        "        result2=f(x)    #f(x-h)\n",
        "\n",
        "        grad[idx]=(result1-result2)/(2*h) #f(x+h)-f(x-h)/2h\n",
        "        x[idx]=temp                       #값 복원\n",
        "    return grad                           #[x[0]에 대한 미분값, x[1]에 대한 미분값]\n",
        "\n",
        "# gradient descent(경사하강법)\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x                            #초기값\n",
        "\n",
        "    for i in range(step_num):\n",
        "        grad=numerical_diff(f,x)          #초기값에대한 기울기\n",
        "        x=x-lr*grad                       #[x0-(learning_rate*grad[0]),x1-(learning_rate*grad[1])]\n",
        "    return x\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3)     #가우스 정규분포로 초기화\n",
        "    def predict(self,x):                  #입력값*가중치\n",
        "        return np.dot(x,self.W)\n",
        "    def loss(self,x,t):\n",
        "        z=self.predict(x)                 #입력값*가중치\n",
        "        y=softmax(z)                      #softmax(입력값*가중치)\n",
        "        result=cross_entropy_error(y,t)   #교차 엔트로피를 손실함수로 사용\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "\n",
        "        self.params={}\n",
        "        self.params['W1']=weight_init_std * np.random.randn(input_size,hidden_size)  #가중치 w1\n",
        "        self.params['b1']=np.zeros(hidden_size)                                      #편향 b1\n",
        "        self.params['W2']=weight_init_std * np.random.randn(hidden_size, output_size)#가중치 w2\n",
        "        self.params['b2'] = np.zeros(output_size)                                    #편향 b2\n",
        "\n",
        "    def predict(self,x):                                                             #순전파\n",
        "        W1,W2=self.params['W1'],self.params['W2']\n",
        "        b1,b2=self.params['b1'],self.params['b2']\n",
        "\n",
        "        a1=np.dot(x,W1)+b1\n",
        "        z1=sigmoid(a1)\n",
        "        a2=np.dot(z1,W2)+b2\n",
        "        y=softmax(a2)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def loss(self, x,t):\n",
        "        y=self.predict(x)                                                               #순전파\n",
        "        return cross_entropy_error(y,t)                                                #손실함수 적용\n",
        "\n",
        "    def accuracy(self,x,t):\n",
        "        y=self.predict(x)                                                               #순전파\n",
        "        y=np.argmax(y,axis=1)                                                          #가장큰값의 인덱스 출력(가로기준)\n",
        "        t=np.argmax(t,axis=1)                                                          #가장큰값의 인덱스 출력(가로기준)\n",
        "\n",
        "        accuracy = np.sum(y==t) / float(x.shape[0])                                    #일치된 정답의 수/전체데이터 수\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self,x,t):\n",
        "        loss_W= lambda W: self.loss(x,t)                                               #손실함수 결과값 얻음\n",
        "        grads={}\n",
        "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])                       #(손실함수 결과, 가중치값) 즉, 각 가중치값에 대하여 손실함수 값을 편미분한다.\n",
        "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPE8HnV3Fh6U"
      },
      "source": [
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "from test import TwoLayerNet\n",
        "\n",
        "(x_train,t_train),(x_test,t_test)=load_mnist(normalize=True, one_hot_label=True)  #train 데이터와 test데이터분리\n",
        "\n",
        "train_loss_list=[]\n",
        "\n",
        "iters=10000                                                                       #반복횟수\n",
        "train_size = x_train.shape[0]                                                     #데이터 크기\n",
        "batch_size = 100                                                                  #배치사이즈\n",
        "lr = 0.1                                                                          #learning rate\n",
        "\n",
        "network= TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters):\n",
        "    batch_mask=np.random.choice(train_size,batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grad = network.numerical_gradient(x_batch,t_batch)                            #기울기 계산\n",
        "\n",
        "    for key in ('W1','b1','W2','b2'):\n",
        "        network.params[key]=network.params[key] - lr*grad[key]                    #경사하강법적용\n",
        "\n",
        "    result = network.loss(x_batch,t_batch)\n",
        "    train_loss_list.append(result)                                                #반복문에서 i번재 마다의 손실함수 값 저장\n",
        "\n",
        "\n",
        "print(train_loss_list.head(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}